{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Matthew's CNN Notebook\n\n## Overview\nIn this notebook I'll be creating a baseline CNN, and iterating off of that model.","metadata":{}},{"cell_type":"markdown","source":"## Preparing the Data\n","metadata":{}},{"cell_type":"code","source":"# Import statements\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport keras\n\n# Instantiating a generator object and normalizing the RGB values\ntraingen = keras.preprocessing.image.ImageDataGenerator(rescale=1/255)\ntestgen = keras.preprocessing.image.ImageDataGenerator(rescale=1/255)\nvalgen = keras.preprocessing.image.ImageDataGenerator(rescale=1/255)\n\n# Creating the generator for the training data\ntrain_data = traingen.flow_from_directory(\n    # Specifying location of training data\n    directory='../input/chest-xray-pneumonia/chest_xray/train',\n    # Re-sizing images to 150x150\n    target_size=(150, 150),\n    # Class mode to binary to recoginize the two directories \"NORMAL\" and \"PNEUMONIA\" as the labels\n    class_mode='binary',\n    batch_size=20,\n    seed=42\n)\n# Creating the generator for the testing data\ntest_data = testgen.flow_from_directory(\n    # Specifying location of testing data\n    directory='../input/chest-xray-pneumonia/chest_xray/test',\n    # Re-sizing images to 150x150\n    target_size=(150, 150),\n    # Class mode to binary to recoginize the two directories \"NORMAL\" and \"PNEUMONIA\" as the labels\n    class_mode='binary',\n    batch_size=20,\n    seed=42\n)\n\n# Setting aside a validation set\nval_data = valgen.flow_from_directory(\n    # Specifying location of testing data\n    directory='../input/chest-xray-pneumonia/chest_xray/val',\n    # Re-sizing images to 150x150\n    target_size=(150, 150),\n    # Class mode to binary to recoginize the two directories \"NORMAL\" and \"PNEUMONIA\" as the labels\n    class_mode='binary',\n    batch_size=20,\n    seed=42\n)","metadata":{"execution":{"iopub.status.busy":"2021-11-17T16:36:18.972113Z","iopub.execute_input":"2021-11-17T16:36:18.972491Z","iopub.status.idle":"2021-11-17T16:36:26.885234Z","shell.execute_reply.started":"2021-11-17T16:36:18.972416Z","shell.execute_reply":"2021-11-17T16:36:26.883737Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## Baseline CNN","metadata":{}},{"cell_type":"code","source":"# Create model\nbase_cnn = keras.Sequential()\n\n# Add single Conv2D and MaxPool layer\nbase_cnn.add(keras.layers.Conv2D(32, (2, 2), activation='relu', input_shape=(150, 150, 3)))\nbase_cnn.add(keras.layers.MaxPool2D(2, 2))\n\nbase_cnn.add(keras.layers.Flatten())\nbase_cnn.add(keras.layers.Dense(1, 'sigmoid'))\n\n\n#Compile model\nbase_cnn.compile(\n    loss='binary_crossentropy',\n    optimizer='sgd',\n    metrics=['acc']\n    \n)\n\n# Fit Model to Training\nbase_cnn_results = base_cnn.fit_generator(train_data,\n                              steps_per_epoch=100,\n                              epochs=10,\n                              validation_data=test_data\n)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-11-17T14:58:10.081319Z","iopub.execute_input":"2021-11-17T14:58:10.081593Z","iopub.status.idle":"2021-11-17T15:05:06.942857Z","shell.execute_reply.started":"2021-11-17T14:58:10.081556Z","shell.execute_reply":"2021-11-17T15:05:06.942011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Conclusion\nThis is a good result for a first baseline model, but some obvious issues just from looking at these results:\n\n- The model is overfitting\n- Validation accuracy is bouncing all over the place, instead of consistently improving.\n\nThere are several things that could be done from here, so let's move on to something a little more robust.","metadata":{}},{"cell_type":"markdown","source":"## Deeper CNN\n\nTo start, I'm just going to add more layers to the network.","metadata":{}},{"cell_type":"code","source":"# Create model\ndeep_cnn = keras.Sequential()\n\n# Adding first Conv2D and MaxPool layer, starting small and then growing larger.\ndeep_cnn.add(keras.layers.Conv2D(32, (2, 2), activation='relu', input_shape=(150, 150, 3)))\ndeep_cnn.add(keras.layers.MaxPool2D(2, 2))\n\n# Second layer with 64 filters\ndeep_cnn.add(keras.layers.Conv2D(64, (2, 2), activation='relu'))\ndeep_cnn.add(keras.layers.MaxPool2D(2, 2))\n\n# Third layer with 96 filters\ndeep_cnn.add(keras.layers.Conv2D(96, (2, 2), activation='relu'))\ndeep_cnn.add(keras.layers.MaxPool2D(2, 2))\n# Flatten layers, and add Densley connected layers for prediction\ndeep_cnn.add(keras.layers.Flatten())\n\n# Dense layer with 32 nodes\ndeep_cnn.add(keras.layers.Dense(32, activation='relu'))\n\n# Dense layer with 64 nodes\ndeep_cnn.add(keras.layers.Dense(64, activation='relu'))\n\n# Dense layer with 96 nodes\ndeep_cnn.add(keras.layers.Dense(96, activation='relu'))\n\n# Sigmoid output layer\ndeep_cnn.add(keras.layers.Dense(1, 'sigmoid'))\n\n\n#Compile model\ndeep_cnn.compile(\n    loss='binary_crossentropy',\n    optimizer='sgd',\n    # Adding additonal metrics for better monitoring of training.\n    metrics=['acc', 'Recall', 'Precision']\n    \n)\n\n# Fit Model to Training\ndeep_cnn_results = deep_cnn.fit_generator(train_data,\n                              steps_per_epoch=100,\n                              epochs=10,\n                              validation_data=test_data)","metadata":{"execution":{"iopub.status.busy":"2021-11-17T15:43:23.199733Z","iopub.execute_input":"2021-11-17T15:43:23.200192Z","iopub.status.idle":"2021-11-17T15:50:36.665515Z","shell.execute_reply.started":"2021-11-17T15:43:23.200127Z","shell.execute_reply":"2021-11-17T15:50:36.664756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Conclusion\nI added additional metrics on this model for more insights into the results of the training proccess. As far as performance goes it's definetly an improvement from the last model in terms of validation accuracy.\n\nSome other notes about the model:\n- The model is still overfitting\n- The validation accuracy is not conistently improving\n- Validation recall is very high, ~97% of true positives were identified correctly. This is good, since we decided that, in context of our buisness problem, false negatives are more costly then false positives.\n\nLets do some tuning to address the overfitting issues.","metadata":{}},{"cell_type":"markdown","source":"### Deeper CNN with Dropout Layers\nI'm going to add dropout layers to the model in order to combat the rampant overfitting in my data.","metadata":{}},{"cell_type":"code","source":"# Create model\nr_cnn = keras.Sequential()\n\n# Adding first Conv2D and MaxPool layer, starting small and then growing larger.\nr_cnn.add(keras.layers.Conv2D(32, (2, 2), activation='relu', input_shape=(150, 150, 3)))\nr_cnn.add(keras.layers.MaxPool2D(2, 2))\n\n# Second layer with 64 filters\nr_cnn.add(keras.layers.Conv2D(64, (2, 2), activation='relu'))\nr_cnn.add(keras.layers.MaxPool2D(2, 2))\n\n# Third layer with 96 filters\nr_cnn.add(keras.layers.Conv2D(96, (2, 2), activation='relu'))\nr_cnn.add(keras.layers.MaxPool2D(2, 2))\n# Flatten layers, and add Densley connected layers for prediction\nr_cnn.add(keras.layers.Flatten())\n\n# Dense layer with 32 nodes with dropout layer\nr_cnn.add(keras.layers.Dense(32, activation='relu'))\nr_cnn.add(keras.layers.Dropout(0.3))\n\n# Dense layer with 64 nodes with dropout layer\nr_cnn.add(keras.layers.Dense(64, activation='relu'))\nr_cnn.add(keras.layers.Dropout(0.3))\n\n# Dense layer with 96 nodes with dropout layer\nr_cnn.add(keras.layers.Dense(96, activation='relu'))\nr_cnn.add(keras.layers.Dropout(0.3))\n# Sigmoid output layer\nr_cnn.add(keras.layers.Dense(1, 'sigmoid'))\n\n\n#Compile model\nr_cnn.compile(\n    loss='binary_crossentropy',\n    optimizer='sgd',\n    # Adding additonal metrics for better monitoring of training.\n    metrics=['acc', 'Recall', 'Precision']\n    \n)\n\n# Fit Model to Training\nr_cnn_results = r_cnn.fit_generator(train_data,\n                              steps_per_epoch=100,\n                              epochs=10,\n                              validation_data=test_data)","metadata":{"execution":{"iopub.status.busy":"2021-11-17T16:36:26.887180Z","iopub.execute_input":"2021-11-17T16:36:26.887500Z","iopub.status.idle":"2021-11-17T16:43:43.668332Z","shell.execute_reply.started":"2021-11-17T16:36:26.887458Z","shell.execute_reply":"2021-11-17T16:43:43.667611Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1,2,figsize=(16, 8))\nhistory = r_cnn_results.history\nax1.plot(history['loss'])\nax1.plot(history['val_loss'])\nax1.xaxis.set_label('Epochs')\nax1.yaxis.set_label('Loss')\nax1.legend(['loss', 'val_loss'])\n\n\nax2.plot(history['acc'])\nax2.plot(history['val_acc'])\nax1.xaxis.set_label('Epochs')\nax1.yaxis.set_label('Accuracy')\nax2.legend(['Accuracy', 'Val_acc'])\n\nfig.suptitle('Loss and Accuracy of Model');","metadata":{"execution":{"iopub.status.busy":"2021-11-17T17:01:27.954415Z","iopub.execute_input":"2021-11-17T17:01:27.954971Z","iopub.status.idle":"2021-11-17T17:01:28.325905Z","shell.execute_reply.started":"2021-11-17T17:01:27.954934Z","shell.execute_reply":"2021-11-17T17:01:28.325266Z"},"trusted":true},"execution_count":25,"outputs":[]}]}