{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Matthew's CNN Notebook\n\n## Overview\nIn this notebook I'll be creating a baseline CNN, and iterating off of that model.","metadata":{}},{"cell_type":"markdown","source":"## Preparing the Data\n","metadata":{}},{"cell_type":"code","source":"# Import statements\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport keras\nimport seaborn as sns\nfrom tensorflow.keras.optimizers import SGD\n# Instantiating a generator object and normalizing the RGB values\ntraingen = keras.preprocessing.image.ImageDataGenerator(rescale=1/255)\ntestgen = keras.preprocessing.image.ImageDataGenerator(rescale=1/255)\nvalgen = keras.preprocessing.image.ImageDataGenerator(rescale=1/255)\n\n# Creating the generator for the training data\ntrain_data = traingen.flow_from_directory(\n    # Specifying location of training data\n    directory='../input/chest-xray-pneumonia/chest_xray/train',\n    # Re-sizing images to 150x150\n    target_size=(150, 150),\n    # Class mode to binary to recoginize the two directories \"NORMAL\" and \"PNEUMONIA\" as the labels\n    class_mode='binary',\n    batch_size=20,\n    seed=42\n)\n# Creating the generator for the testing data\ntest_data = testgen.flow_from_directory(\n    # Specifying location of testing data\n    directory='../input/chest-xray-pneumonia/chest_xray/test',\n    # Re-sizing images to 150x150\n    target_size=(150, 150),\n    # Class mode to binary to recoginize the two directories \"NORMAL\" and \"PNEUMONIA\" as the labels\n    class_mode='binary',\n    batch_size=20,\n    seed=42,\n    shuffle=False\n)\n\n# Setting aside a validation set\nval_data = valgen.flow_from_directory(\n    # Specifying location of testing data\n    directory='../input/chest-xray-pneumonia/chest_xray/val',\n    # Re-sizing images to 150x150\n    target_size=(150, 150),\n    # Class mode to binary to recoginize the two directories \"NORMAL\" and \"PNEUMONIA\" as the labels\n    class_mode='binary',\n    batch_size=20,\n    seed=42\n)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T15:35:57.194659Z","iopub.execute_input":"2021-11-18T15:35:57.194908Z","iopub.status.idle":"2021-11-18T15:35:58.265893Z","shell.execute_reply.started":"2021-11-18T15:35:57.194880Z","shell.execute_reply":"2021-11-18T15:35:58.265097Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"## Baseline CNN","metadata":{}},{"cell_type":"code","source":"# Create model\nbase_cnn = keras.Sequential()\n\n# Add single Conv2D and MaxPool layer\nbase_cnn.add(keras.layers.Conv2D(32, (2, 2), activation='relu', input_shape=(150, 150, 3)))\nbase_cnn.add(keras.layers.MaxPool2D(2, 2))\n\nbase_cnn.add(keras.layers.Flatten())\nbase_cnn.add(keras.layers.Dense(1, 'sigmoid'))\n\n\n#Compile model\nbase_cnn.compile(\n    loss='binary_crossentropy',\n    optimizer='sgd',\n    metrics=['acc']\n    \n)\n\n# Fit Model to Training\nbase_cnn_results = base_cnn.fit_generator(train_data,\n                              steps_per_epoch=100,\n                              epochs=10,\n                              validation_data=test_data\n)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-11-17T14:58:10.081319Z","iopub.execute_input":"2021-11-17T14:58:10.081593Z","iopub.status.idle":"2021-11-17T15:05:06.942857Z","shell.execute_reply.started":"2021-11-17T14:58:10.081556Z","shell.execute_reply":"2021-11-17T15:05:06.942011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Conclusion\nThis is a good result for a first baseline model, but some obvious issues just from looking at these results:\n\n- The model is overfitting\n- Validation accuracy is bouncing all over the place, instead of consistently improving.\n\nThere are several things that could be done from here, so let's move on to something a little more robust.","metadata":{}},{"cell_type":"markdown","source":"## Deeper CNN\n\nTo start, I'm just going to add more layers to the network.","metadata":{}},{"cell_type":"code","source":"# Create model\ndeep_cnn = keras.Sequential()\n\n# Adding first Conv2D and MaxPool layer, starting small and then growing larger.\ndeep_cnn.add(keras.layers.Conv2D(32, (2, 2), activation='relu', input_shape=(150, 150, 3)))\ndeep_cnn.add(keras.layers.MaxPool2D(2, 2))\n\n# Second layer with 64 filters\ndeep_cnn.add(keras.layers.Conv2D(64, (2, 2), activation='relu'))\ndeep_cnn.add(keras.layers.MaxPool2D(2, 2))\n\n# Third layer with 96 filters\ndeep_cnn.add(keras.layers.Conv2D(96, (2, 2), activation='relu'))\ndeep_cnn.add(keras.layers.MaxPool2D(2, 2))\n# Flatten layers, and add Densley connected layers for prediction\ndeep_cnn.add(keras.layers.Flatten())\n\n# Dense layer with 32 nodes\ndeep_cnn.add(keras.layers.Dense(32, activation='relu'))\n\n# Dense layer with 64 nodes\ndeep_cnn.add(keras.layers.Dense(64, activation='relu'))\n\n# Dense layer with 96 nodes\ndeep_cnn.add(keras.layers.Dense(96, activation='relu'))\n\n# Sigmoid output layer\ndeep_cnn.add(keras.layers.Dense(1, 'sigmoid'))\n\n\n#Compile model\ndeep_cnn.compile(\n    loss='binary_crossentropy',\n    optimizer='sgd',\n    # Adding additonal metrics for better monitoring of training.\n    metrics=['acc', 'Recall', 'Precision']\n    \n)\n\n# Fit Model to Training\ndeep_cnn_results = deep_cnn.fit_generator(train_data,\n                              steps_per_epoch=100,\n                              epochs=10,\n                              validation_data=test_data)","metadata":{"execution":{"iopub.status.busy":"2021-11-17T15:43:23.199733Z","iopub.execute_input":"2021-11-17T15:43:23.200192Z","iopub.status.idle":"2021-11-17T15:50:36.665515Z","shell.execute_reply.started":"2021-11-17T15:43:23.200127Z","shell.execute_reply":"2021-11-17T15:50:36.664756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Conclusion\nI added additional metrics on this model for more insights into the results of the training proccess. As far as performance goes it's definetly an improvement from the last model in terms of validation accuracy.\n\nSome other notes about the model:\n- The model is still overfitting\n- The validation accuracy is not conistently improving\n- Validation recall is very high, ~97% of true positives were identified correctly. This is good, since we decided that, in context of our buisness problem, false negatives are more costly then false positives.\n\nLets do some tuning to address the overfitting issues.","metadata":{}},{"cell_type":"markdown","source":"### Deeper CNN with Dropout Layers\nI'm going to add dropout layers to the model in order to combat the rampant overfitting in my data.","metadata":{}},{"cell_type":"code","source":"# Create model\nr_cnn = keras.Sequential()\n\n# Adding first Conv2D and MaxPool layer, starting small and then growing larger.\nr_cnn.add(keras.layers.Conv2D(32, (2, 2), activation='relu', input_shape=(150, 150, 3)))\nr_cnn.add(keras.layers.MaxPool2D(2, 2))\n\n# Second layer with 64 filters\nr_cnn.add(keras.layers.Conv2D(64, (2, 2), activation='relu'))\nr_cnn.add(keras.layers.MaxPool2D(2, 2))\n\n# Third layer with 96 filters\nr_cnn.add(keras.layers.Conv2D(96, (2, 2), activation='relu'))\nr_cnn.add(keras.layers.MaxPool2D(2, 2))\n# Flatten layers, and add Densley connected layers for prediction\nr_cnn.add(keras.layers.Flatten())\n\n# Dense layer with 32 nodes with dropout layer\nr_cnn.add(keras.layers.Dense(32, activation='relu'))\nr_cnn.add(keras.layers.Dropout(0.3))\n\n# Dense layer with 64 nodes with dropout layer\nr_cnn.add(keras.layers.Dense(64, activation='relu'))\nr_cnn.add(keras.layers.Dropout(0.3))\n\n# Dense layer with 96 nodes with dropout layer\nr_cnn.add(keras.layers.Dense(96, activation='relu'))\nr_cnn.add(keras.layers.Dropout(0.3))\n# Sigmoid output layer\nr_cnn.add(keras.layers.Dense(1, 'sigmoid'))\n\n\n#Compile model\nr_cnn.compile(\n    loss='binary_crossentropy',\n    optimizer='sgd',\n    # Adding additonal metrics for better monitoring of training.\n    metrics=['acc', 'Recall', 'Precision']\n    \n)\n\n# Fit Model to Training\nr_cnn_results = r_cnn.fit_generator(train_data,\n                              steps_per_epoch=100,\n                              epochs=10,\n                              validation_data=test_data)","metadata":{"execution":{"iopub.status.busy":"2021-11-17T16:36:26.88718Z","iopub.execute_input":"2021-11-17T16:36:26.8875Z","iopub.status.idle":"2021-11-17T16:43:43.668332Z","shell.execute_reply.started":"2021-11-17T16:36:26.887458Z","shell.execute_reply":"2021-11-17T16:43:43.667611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclusion","metadata":{}},{"cell_type":"code","source":"# Visualizing results\n\n# Creating figure with 2 subplots\nfig, (ax1, ax2) = plt.subplots(1,2,figsize=(16, 8))\n\n# Geting training history from results\nhistory = r_cnn_results.history\n\n# Ploting on first subplot\nax1.plot(history['loss'])\nax1.plot(history['val_loss'])\n\n# Labeling \nax1.xaxis.set_label('Epochs')\nax1.yaxis.set_label('Loss')\nax1.legend(['loss', 'val_loss'])\n\n# Ploting on second subplot\nax2.plot(history['acc'])\nax2.plot(history['val_acc'])\n\n# Labeling \nax1.xaxis.set_label('Epochs')\nax1.yaxis.set_label('Accuracy')\nax2.legend(['Accuracy', 'Val_acc'])\n\nfig.suptitle('Loss and Accuracy of Model');","metadata":{"execution":{"iopub.status.busy":"2021-11-17T17:01:27.954415Z","iopub.execute_input":"2021-11-17T17:01:27.954971Z","iopub.status.idle":"2021-11-17T17:01:28.325905Z","shell.execute_reply.started":"2021-11-17T17:01:27.954934Z","shell.execute_reply":"2021-11-17T17:01:28.325266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The dropout layer definetly reduced the overfitting that was occuring. Though still some other issues with this model, specifically, **the validation loss sharply rises in the last few epochs**. I'm going to add early stopping and checkpoints to the model to help with this issue.","metadata":{}},{"cell_type":"markdown","source":"## CNN with Early Stopping and More Training\nSince I'm implementing early stopping, I'll also add some epochs / steps to the training process","metadata":{}},{"cell_type":"code","source":"# Create early stopping object\nearly_stopping = [\n    keras.callbacks.EarlyStopping(monitor='val_loss', patience=3),\n    keras.callbacks.ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.2f}.h5')\n]\n\n# Create model\nes_cnn = keras.Sequential()\n\n# Adding first Conv2D and MaxPool layer, starting small and then growing larger.\nes_cnn.add(keras.layers.Conv2D(32, (2, 2), activation='relu', input_shape=(150, 150, 3)))\nes_cnn.add(keras.layers.MaxPool2D(2, 2))\n\n# Second layer with 64 filters\nes_cnn.add(keras.layers.Conv2D(64, (3, 3), activation='relu'))\nes_cnn.add(keras.layers.MaxPool2D(3, 3))\n\n# Third layer with 96 filters\nes_cnn.add(keras.layers.Conv2D(96, (5, 5), activation='relu'))\nes_cnn.add(keras.layers.MaxPool2D(5, 5))\n# Flatten layers, and add Densley connected layers for prediction\nes_cnn.add(keras.layers.Flatten())\n\n# Dense layer with 32 nodes with dropout layer\nes_cnn.add(keras.layers.Dense(32, activation='relu'))\nes_cnn.add(keras.layers.Dropout(0.3))\n\n# Dense layer with 64 nodes with dropout layer\nes_cnn.add(keras.layers.Dense(64, activation='relu'))\nes_cnn.add(keras.layers.Dropout(0.3))\n\n# Dense layer with 96 nodes with dropout layer\nes_cnn.add(keras.layers.Dense(96, activation='relu'))\nes_cnn.add(keras.layers.Dropout(0.3))\n# Sigmoid output layer\nes_cnn.add(keras.layers.Dense(1, 'sigmoid'))\n\n\n#Compile model\nes_cnn.compile(\n    loss='binary_crossentropy',\n    optimizer='sgd',\n    # Adding additonal metrics for better monitoring of training.\n    metrics=['acc', 'Recall', 'Precision']\n    \n)\n\n# Fit Model to Training\nes_cnn_results = es_cnn.fit_generator(train_data,\n                              steps_per_epoch=150,\n                              epochs=25,\n                              validation_data=test_data,\n                              callbacks=early_stopping)","metadata":{"execution":{"iopub.status.busy":"2021-11-17T20:16:02.643806Z","iopub.execute_input":"2021-11-17T20:16:02.644075Z","iopub.status.idle":"2021-11-17T20:16:06.247859Z","shell.execute_reply.started":"2021-11-17T20:16:02.644044Z","shell.execute_reply":"2021-11-17T20:16:06.245378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclusion\n\nEarly stopping is working as intended, however, I've noticed that the first few epochs always have the same validation accuracy: 0.6250.\n\nThe model may be finding the local minimim instead of the global in these epochs. I'll try to tune the learning rate of my optimizer, and seeeing if that changes anything. I will also try to introduce class weights to the model, as that may help with the problem as well.","metadata":{}},{"cell_type":"markdown","source":"# Optimizer and Misc Tuning","metadata":{}},{"cell_type":"code","source":"#### # Create model\nop_cnn = keras.Sequential()\n\n# Adding first Conv2D and MaxPool layer, starting small and then growing larger.\nop_cnn.add(keras.layers.Conv2D(32, (2, 2), activation='relu', input_shape=(150, 150, 3)))\nop_cnn.add(keras.layers.MaxPool2D(2, 2))\n\n# Second layer with 64 filters\nop_cnn.add(keras.layers.Conv2D(64, (3, 3), activation='relu'))\nop_cnn.add(keras.layers.MaxPool2D(3, 3))\n\n# Third layer with 96 filters\nop_cnn.add(keras.layers.Conv2D(96, (5, 5), activation='relu'))\nop_cnn.add(keras.layers.MaxPool2D(5, 5))\n# Flatten layers, and add Densley connected layers for prediction\nop_cnn.add(keras.layers.Flatten())\n\n# Dense layer with 32 nodes with dropout layer\nop_cnn.add(keras.layers.Dense(32, activation='relu'))\nop_cnn.add(keras.layers.Dropout(0.3))\n\n# Dense layer with 64 nodes with dropout layer\nop_cnn.add(keras.layers.Dense(64, activation='relu'))\nop_cnn.add(keras.layers.Dropout(0.3))\n\n# Dense layer with 96 nodes with dropout layer\nop_cnn.add(keras.layers.Dense(96, activation='relu'))\nop_cnn.add(keras.layers.Dropout(0.3))\n# Sigmoid output layer\nop_cnn.add(keras.layers.Dense(1, 'sigmoid'))\n\n# Create early stopping object\nop_early_stopping = [\n    keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n    keras.callbacks.ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.2f}.h5')\n]\n\n# Create optimizer\noptim = SGD(learning_rate=0.001, momentum=0.9, nesterov=True)\n\n# Creating class weights\nweights = {\n    0: 2.88, # NORMAL\n    1: 1.    # PNEM\n}\n#Compile model\nop_cnn.compile(\n    loss='binary_crossentropy',\n    optimizer=optim,\n    # Adding additonal metrics for better monitoring of training.\n    metrics=['acc', 'Recall', 'Precision']\n    \n)\n\n# Fit Model to Training\nop_cnn_results = op_cnn.fit_generator(train_data,\n                              class_weight=weights,\n                              steps_per_epoch=50,\n                              epochs=100,\n                              validation_data=test_data,\n                              callbacks=op_early_stopping,)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T14:51:32.030235Z","iopub.execute_input":"2021-11-18T14:51:32.030759Z","iopub.status.idle":"2021-11-18T14:56:55.505428Z","shell.execute_reply.started":"2021-11-18T14:51:32.030723Z","shell.execute_reply":"2021-11-18T14:56:55.504655Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"op_cnn.evaluate(test_data)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T14:57:07.252062Z","iopub.execute_input":"2021-11-18T14:57:07.252877Z","iopub.status.idle":"2021-11-18T14:57:26.852860Z","shell.execute_reply.started":"2021-11-18T14:57:07.252838Z","shell.execute_reply":"2021-11-18T14:57:26.852126Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Conclusion","metadata":{"execution":{"iopub.status.busy":"2021-11-17T21:16:48.591717Z","iopub.execute_input":"2021-11-17T21:16:48.592391Z","iopub.status.idle":"2021-11-17T21:16:48.59642Z","shell.execute_reply.started":"2021-11-17T21:16:48.592353Z","shell.execute_reply":"2021-11-17T21:16:48.595724Z"}}},{"cell_type":"code","source":"# Visualizing results\n\n# Creating figure with 2 subplots\nfig, (ax1, ax2) = plt.subplots(1,2,figsize=(16, 8))\n\n# Geting training history from results\nhistory = op_cnn_results.history\n\n# Ploting on first subplot\nax1.plot(history['loss'])\nax1.plot(history['val_loss'])\n\n# Labeling \nax1.xaxis.set_label('Epochs')\nax1.yaxis.set_label('Loss')\nax1.legend(['loss', 'val_loss'])\n\n# Ploting on second subplot\nax2.plot(history['acc'])\nax2.plot(history['val_acc'])\n\n# Labeling \nax1.xaxis.set_label('Epochs')\nax1.yaxis.set_label('Accuracy')\nax2.legend(['Accuracy', 'Val_acc'])\n\nfig.suptitle('Loss and Accuracy of Model');","metadata":{"execution":{"iopub.status.busy":"2021-11-18T16:23:04.616958Z","iopub.execute_input":"2021-11-18T16:23:04.617220Z","iopub.status.idle":"2021-11-18T16:23:04.633476Z","shell.execute_reply.started":"2021-11-18T16:23:04.617193Z","shell.execute_reply":"2021-11-18T16:23:04.632611Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"The early stopping worked great this time, and the changes to the optimizer, as well as adding class weights,has had a positive impact on the model. Validation accuracy is now sitting around ~87%. I'm going to add more layers and padding to the conv layers next, and see if that helps.","metadata":{}},{"cell_type":"markdown","source":"# ROBUST CNN","metadata":{}},{"cell_type":"code","source":"# Create model\nrob_cnn = keras.Sequential()\n\n# Adding first Conv2D and MaxPool layer, starting small and then growing larger.\nrob_cnn.add(keras.layers.Conv2D(32, (2, 2), padding='same', activation='relu', input_shape=(150, 150, 3)))\nrob_cnn.add(keras.layers.MaxPool2D(2, 2))\n\nrob_cnn.add(keras.layers.Conv2D(32, (2, 2), padding='same', activation='relu'))\nrob_cnn.add(keras.layers.MaxPool2D(2, 2))\n# Second layer with 64 filters\nrob_cnn.add(keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu'))\nrob_cnn.add(keras.layers.MaxPool2D(3, 3))\n\n# Second layer with 64 filters\nrob_cnn.add(keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu'))\nrob_cnn.add(keras.layers.MaxPool2D(3, 3))\n\n# Third layer with 96 filters\nrob_cnn.add(keras.layers.Conv2D(96, (5, 5), padding='same', activation='relu'))\nrob_cnn.add(keras.layers.MaxPool2D(3, 3))\n\n\n# Flatten layers, and add Densley connected layers for prediction\nrob_cnn.add(keras.layers.Flatten())\n\n# Dense layer with 32 nodes with dropout layer\n\nrob_cnn.add(keras.layers.Dense(32, activation='relu'))\nrob_cnn.add(keras.layers.Dropout(0.3))\n\nrob_cnn.add(keras.layers.Dense(32, activation='relu'))\nrob_cnn.add(keras.layers.Dropout(0.3))\n# Dense layer with 64 nodes with dropout layer\nrob_cnn.add(keras.layers.Dense(64, activation='relu'))\nrob_cnn.add(keras.layers.Dropout(0.3))\n\nrob_cnn.add(keras.layers.Dense(64, activation='relu'))\nrob_cnn.add(keras.layers.Dropout(0.3))\n\n# Dense layer with 96 nodes with dropout layer\nrob_cnn.add(keras.layers.Dense(96, activation='relu'))\nrob_cnn.add(keras.layers.Dropout(0.3))\n\nrob_cnn.add(keras.layers.Dense(96, activation='relu'))\nrob_cnn.add(keras.layers.Dropout(0.3))\n# Sigmoid output layer\nrob_cnn.add(keras.layers.Dense(1, 'sigmoid'))\n\n# Create early stopping object\nrob_early_stopping = [\n    keras.callbacks.EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True),\n    keras.callbacks.ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.2f}.h5')\n]\n\n# Create optimizer\noptim = SGD(learning_rate=0.001, momentum=0.9, nesterov=True)\n\n# Creating class weights\nweights = {\n    0: 2.88, # NORMAL\n    1: 1.    # PNEM\n}\n#Compile model\nrob_cnn.compile(\n    loss='binary_crossentropy',\n    optimizer='sgd',\n    # Adding additonal metrics for better monitoring of training.\n    metrics=['acc', 'Recall', 'Precision']\n    \n)\n\n# Fit Model to Training\nrob_cnn_results = rob_cnn.fit_generator(train_data,\n                              class_weight=weights,\n                              steps_per_epoch=75,\n                              epochs=100,\n                              validation_data=test_data,\n                              callbacks=rob_early_stopping,)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T14:06:49.445284Z","iopub.execute_input":"2021-11-18T14:06:49.445565Z","iopub.status.idle":"2021-11-18T14:14:53.798329Z","shell.execute_reply.started":"2021-11-18T14:06:49.445533Z","shell.execute_reply":"2021-11-18T14:14:53.797647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclusion\nIn the end, it does not look like these things have improved the model, so the previous model will be our final model for now.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}